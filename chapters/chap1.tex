\chapter{Euclidean Spaces}
\section{Smooth Functions on a Euclidean Space}
\begin{problem}
\end{problem}

\begin{proof}
	$$g(x) = \int_{0}^{x} t^{1/3}dt = \frac{3}{4}x^4/3$$
	$$g'(x) = x^{1/3}$$
	So, as seen before, $g'$ is $C^0$ but not $C^1$. As such, $g$ is $C^1$, but not $C^2$. And
	$h = \int_{0}^{x} g(t) dt$, which has $h' = g$, is $C^2$ but not $C^3$.
\end{proof}

\begin{problem}
\end{problem}

\begin{enumerate}[label=(\alph*)]
	\item
	      \begin{proof}
		      Base case: $k = 0$, it is obviously true, with $p_{0} = 1$. Suppose it's true for $k > 1$. Then
		      $$f^{(k)}(x) = p_{2k}(1/x)e^{-1/x} $$

		      \begin{align*}
			      f^{(k+1)}(x) & = (p_{2k}(1/x))' \cdot (e^{-1/x}) + (p_{2k}(1/x))\cdot(e^{-1/x})'                     \\
			                   & = (p_{2k})'(1/x)\frac{1}{x^2}\cdot(e^{-1/x}) + p_{2k}(1/x)\cdot e^{-1/x}\frac{1}{x^2} \\
			                   & =  e^{-1/x} \cdot \frac{(p_{2k})'(1/x) + p_{2k}(1/x)}{x^2}                            \\
		      \end{align*}
		      Now, $(p_{2k})'(1/x)/x^2$ is a polynomial on $(1/x)$ of degree $2k + 1$ and $p_{2k}(1/x)/x^2$ is of degree
		      $2k + 2$, so $(p_{2k})'(1/x)/x^2 + p_{2k}(1/x)/x^2$ is a polynomial on $(1/x)$ of degree $2k + 2$, proving the hypothesis.
	      \end{proof}
	\item \begin{proof}
		      These formula are certainly valid for any $x \neq 0$. For $x \to 0$, it suffices to notice that $e^{-1/x} \lll p_{2k}(1/x)$ for any $k$.
		      So $f^{k}$ is defined for all $\R$ and, (taking the limit) is $0$ at $0$ for any $k$.
	      \end{proof}
\end{enumerate}

\begin{problem}
\end{problem}

\begin{enumerate}[label=(\alph*)]
	\item \begin{proof}
		      $\tan$ is $C^\infty$ on $(-\pi/2,\pi/2)$ as, taking derivatives, on the denominators only $\cos$ appears and they never 0
		      on this interval. Its inverse, $\arctan$ has derivative $\frac{1}{1+x^2}$ which
		      also is $C^\infty$.
	      \end{proof}
	\item \begin{proof}
		      Consider
		      $$h(x) = \frac{x - (b+a)/2}{(b-a)/2}$$
	      \end{proof}
	\item \begin{proof}
		      Consider $h(x) = \exp(x) + a$ and $g(x) = b - \exp(x)$, then clearly $h$ and $g$ are diffeomorphisms, and we may compose
		      the inverses to find that by the diffeomorphism $g\circ h^{-1}$, the intervals are diffeomorphic.
	      \end{proof}
\end{enumerate}

\begin{problem}
\end{problem}
\begin{proof}
	Consider the smooth inverse
	$$g : \R^n \to \bigg(-\frac{\pi}{2}, \frac{\pi}{2}\bigg)^n,\quad g(x_1, \dots, x_n) = (\arctan(x_1), \dots, \arctan(x_n))$$
\end{proof}

\begin{problem}
\end{problem}
\begin{enumerate}[label=(\alph*)]
	\item \begin{proof}
		      We parametrize the line between $(0,0,1)$ and $(a,b,c)$ by $t$ and solve for when $z = 0$.
		      $$l(t) = (0,0,1) + t \cdot ((a,b,c) - (0,0,1)) = (ta, tb, 1 + t(c-1))$$
		      $$l_3(t) = 0 \iff 1 + t(c-1) = 0 \iff t = \frac{1}{1-c}$$
		      yielding precisely
		      $$g(a,b,c) = \bigg( \frac{a}{1-c}, \frac{b}{1-c} \bigg)$$
		      as $(a,b,c) \in S$, we know that $c = 1 - \sqrt{1 - a^2 - b^2}$.

		      For the inverse, we procede the same way, solving the line equation for when $|l(t) - (0,0,1)| = 1$.
		      This time it is given by:
		      $$l(t) = (0,0,1) + t\cdot ((x,y,0) - (0,0,1))$$
		      So, for $|l(t) - (0,0,1)| = 1$ to happen, we must have:
		      $$t^2x^2 +t^2y^2 +t^2 = 1 \iff t^2(x^2 + y^2 + 1) = 1$$
		      yielding $t = \pm 1/\sqrt{x^2 + y^2  +1}$, as we know our solution is in the lower hemisphere, we have
		      $t = 1/\sqrt{x^2 + y^2  +1}$. Substituting back on the line equation we find
		      $$(a,b,c) = \bigg(\frac{x}{\sqrt{x^2 + y^2 + 1}}, \frac{y}{\sqrt{x^2 + y^2 + 1}}, 1 - \frac{1}{\sqrt{x^2 + y^2 + 1}}\bigg)$$
	      \end{proof}
	\item \begin{proof}
		      $h^{-1} = f^{-1} \circ g^{-1}$, $g^{-1}$ was found in the previous item, and $f^{-1}$ is simply the projection
		      to the $xy$ plane. So
		      $$h^{-1}(u,v) = \bigg(\frac{u}{\sqrt{1 + u^2 + v^2}}, \frac{v}{\sqrt{1 + u^2 + v^2}} \bigg)$$
		      which is $C^\infty$. $h$ is a diffeomorphism.
	      \end{proof}

	\item \begin{proof}
		      This is the most interesting item, but we do exactly the same thing looking at the $S^n$ dimensional sphere in $R^{n+1}$.
		      Consider the stereographic projection $g: S \to \R^n$ from $(0,0,1)$ given by:
		      $$g(x_1, x_2, \dots, x_n, x_{n+1}) = \bigg(\frac{x_1}{1 - c}, \dots, \frac{x_{n}}{1-c}, 1 - \frac{1}{1 - c}\bigg)$$
		      where $c = 1 - \bigg(\sum_{1}^{n} (x_i)^2\bigg)^{1/2}$
		      Then, following the same construction as before, we find $h$ and $h^{-1}$ where:
		      $$h(x_1,x_2 \dots, x_n) = \bigg(\frac{x_i}{1 - c} \bigg)_{i=1}^{n}$$
		      where the expression on the right is a vector. Similarly, $h^{-1}$ is defined as:
		      $$h^{-1}(x_1,x_2 \dots, x_n) = \Bigg(\frac{x_i}{\sqrt{1 + (x_1)^2 \dots + (x_n)^2}} \Bigg)_{i=1}^{n}$$
	      \end{proof}
\end{enumerate}

\begin{problem}
\end{problem}

\begin{proof}
	We apply Taylor twice. As before, consider the function on the line $f(tx,ty))$. By the chain rule
	$$D_t f(tx,ty) = \partial_x f(tx,ty)x + \partial_y f(tx,ty)y$$
	So, integrating, we find
	$$f(x,y) - f(0,0) = D_t f(tx,ty) \bigg ]_{0}^{1} = x \int_{0}^{1} \partial_x f(tx,ty) dt + y \int_{0}^{1} \partial_y f(tx,ty) dt$$
	$$f(x,y)  = f(0,0) +  x \int_{0}^{1} \partial_x f(tx,ty) dt + y \int_{0}^{1} \partial_y f(tx,ty) dt$$
	Now we do the same for $\partial_x f(tx,ty)$ and $\partial_y f(tx,ty)$, we find:
	$$\partial_x f(x,y)  = \partial_x f(0,0) +  x \int_{0}^{1} \partial_{xx} f(tx,ty) dt + y \int_{0}^{1} \partial_{xy} f(tx,ty) dt$$
	$$\partial_y f(x,y)  = \partial_{y} f(0,0) +  x \int_{0}^{1} \partial_{yx} f(tx,ty) dt + y \int_{0}^{1} \partial_{yy} f(tx,ty) dt$$
	Substituting in the $f(x,y)$ expansion:
	\begin{align*}
		f(x,y)  = f(0,0) + & x \int_{0}^{1}\Bigg( \partial_x f(0,0) +  x \int_{0}^{1} \partial_{xx} f(stx,sty) ds + y \int_{0}^{1} \partial_{xy} f(stx,sty) ds \Bigg) dt \\
		+                  & y \int_{0}^{1}\Bigg( \partial_y f(0,0) +  x \int_{0}^{1} \partial_{yx} f(stx,sty) ds + y \int_{0}^{1} \partial_{yy} f(stx,sty) ds \Bigg) dt \\
		= f(0,0) +         & x \partial_x f(0,0) + y \partial_y f(0,0) + x^2 g_{11}(x,y) + xy g_{12}(x,y) + y^2 g_{22}(x,y)                                              \\
	\end{align*}
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	$g(t,u)$ is 0 at $t = 0$. And, by expanding $f$, we find, for $t \neq 0$:
	\begin{equation*}
		g(t,u) = \frac{1}{t} \bigg( f(0,0)  + \partial_x f(0,0) t + \partial_y f(0,0) tu  +
		t^2 g_{11}(t,tu) + t^2u g_{12}(t,tu) + t^2u^2 g_{22}(t,tu) \bigg)
	\end{equation*}
	Noticing $f(0,0) = \partial_x f(0,0) = \partial_y f(0,0) = 0$ we get:
	$$g(t,u) = t g_{11}(t,tu) + tu g_{12}(tu) + tu^2 g_{22}(t,tu)$$
	Because $g(0,u) = 0$, this formula is valid for $t = 0$ as well, and this expression is $C^\infty$.
\end{proof}

\begin{problem}
$f^{-1} = x^{1/3}$ which is not differentiable at $0$. In complex analysis, as a consequence of Rouche's theorem,
if $f'(z) = 0$, then $f(z + s) = f''(z)s^2 + \dots$, and it can be shown that for sufficiently small $s$, we have at least two solutions.

\end{problem}

\section{Tangent Vectors in Rn as Derivations}

\begin{problem}
\end{problem}
\begin{proof}
	$$ X = x\partial_x + y\partial_y$$
	$$ f(x,y,z) =  x^2 + y^2 + z^2$$
	Then, computing $Xf$ is as simple as applying $X$ to $f$ at every point:
	$$Xf = x\partial_x f + y\partial_y f = 2x^2 + 2y^2$$
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	We define all such operations point-wise on $C_{p}^{\infty}$. For $f,g \in C_{p}^{\infty}$ and $\lambda \in \R$,
	for any $x \in U$ :
	\begin{align*}
		(f + g) (x)     & = f(x) + g(x) = g(x) + f(x) =  (g + f)(x)             \\
		(f \cdot g) (x) & = f(x) \cdot g(x) = g(x) \cdot f(x) =  (g \cdot f)(x) \\
		(\lambda f) (x) & = \lambda \cdot f(x)
	\end{align*}
	Such operations are closed in $C_{p}^{\infty}$ as differentiability is a local property closed under these
	operations.
\end{proof}

\begin{problem}
\end{problem}


\begin{enumerate}[label=(\alph*)]
	\item \begin{proof}
		      Let $D, D'$ be derivation at $p$. Then both $D,D'$ are linear maps of the form $C_p^{\infty} \to \R$, that
		      satisfy the Leibniz rule.
		      $$(D + D')(\lambda f + g) = D(\lambda f + g) + D'(\lambda f + g) = \lambda (D + D')f + (D + D')g$$
		      So $D + D'$ is linear. We also have:
		      $$(D + D')(fg) = D(fg) + D'(fg) = (Df)g + f(Dg) + (D'f)g + f(D'g) = (D + D')(f)g + f(D + D')(g)$$
		      As we wanted to show.
	      \end{proof}
	\item \begin{proof}
		      Certainly is a linear map and the $c$ pops inside the Leibniz rule
		      $$cD(fg) = c((Df)g + f(Dg)) = (cDf)g + f(cDg)$$
	      \end{proof}
\end{enumerate}

\begin{problem}
\end{problem}
\begin{proof}
	Let $D_1,D_2: A \to A$, then $D_1 \circ D_2 : A \to A$. And:
	$$D_1 \circ D_2 (ab) = D_1(a(D_2b)) + D_1((D_2a)b) = (D_1a)(D_2b) + a(D_1D_2b) + (D_1D_2a)b + (D_2a)(D_1b)$$
	which certainly isn't necessairly equal to:
	$$a(D_1(D_2b)) + (D_1(D_2a))b$$

	Now let's consider $D_1 \circ D_2 - D_2 \circ D_1$, which is clearly a linear map.
	\begin{align*}
		(D_1 \circ D_2 - D_2 \circ D_1)(ab) & = (D_1a)(D_2b) + a(D_1D_2b) + (D_1D_2a)b + (D_2a)(D_1b) \\
		                                    & - (D_2a)(D_1b) - a(D_2D_1b) - (D_2D_1a)b - (D_1a)(D_2b) \\
		                                    & = a[D_1D_2 - D_2D_1](b) + [D_1D_2 - D_2D_1](a)b
	\end{align*}
	As we wanted to show.
\end{proof}

\section{The Exterior Algebra of Multivectors}
In some chapters, before the problem section there are some exercises within the text.
\subsection{Within Text Exercises}

\begin{customprob}{3.6}
\end{customprob}


\begin{proof}
	We know looking at it that the inversions are $(2,1), (3,1), (4,1), (5,1)$. But we might have a clearer view writing it
	in matricial form:
	$$
		\begin{bmatrix}
			1 & 2 & 3 & 4 & 5 \\
			2 & 3 & 4 & 5 & 1
		\end{bmatrix}
	$$
\end{proof}

\begin{customprob}{3.13}
\end{customprob}
\begin{proof}
	Consider $\tau Sf$.
	\begin{align*}
		\tau Sf & = \sum_{\sigma \in S_k} \tau(\sigma f) \\
		        & = \sum_{\sigma \in S_k} (\tau\sigma f) \\
		        & = \sum_{\sigma \in S_k} \sigma f
	\end{align*}
	As we have set equality $\{\tau \sigma, \sigma \in S_k\} = \{\sigma \in S_k\}$
\end{proof}


\begin{customprob}{3.15}
\end{customprob}
\begin{proof}
	We write the expression containing the 6 permutations and their signs:
	\begin{align*}
		A f(v_1,v_2,v_3) & = f(v_1,v_2,v_3) + (-1)f(v_1,v_3,v_2) + f(v_2, v_1, v_3)         \\
		                 & + (-1)f(v_2, v_3, v_1) + (-1)f(v_3, v_1, v_2) + f(v_3, v_2, v_1)
	\end{align*}
\end{proof}


\begin{customprob}{3.17}
\end{customprob}
\begin{proof}
	let $f$ be $k$-linear, $g$ $l$-linear and $h$ $m$-linear. Then:
	$$(f \otimes g) \otimes h = (f(v_1, \dots v_k)\cdot g(v_{k+1}, \dots v_{k+l})) \otimes h =
		f(v_1, \dots v_k)\cdot g(v_{k+1}, \dots v_{k+l}) \cdot h(v_{k+l+1}, \dots v_{k+l+m})$$
	Similarly, as $g$ is $l$-linear and $h$ $m$-linear, $(g \otimes h)$ is $l+m$ linear, as such:
	$$f \otimes (g \otimes h) = f(v_1, \dots v_k) \otimes (g\otimes h) =
		f(v_1, \dots v_k) \otimes (g(w_{1}, \dots w_{k+l}) \cdot h(w_{l+1}, \dots w_{l+m})$$
	Resulting in what was expected:
	$$f \otimes (g \otimes h) = f(v_1, \dots v_k)\cdot g(v_{k+1}, \dots v_{k+l}) \cdot h(v_{k+l+1}, \dots v_{k+l+m}) $$
\end{proof}

\begin{customprob}{3.20}
\end{customprob}
\begin{proof}
	$$f \wedge g (v_1,v_2,v_3,v_4) = \sum_{(2,2)-\text{shuffles}\;\sigma} f(v_{\sigma(1)}, v_{\sigma(2)})g(v_{\sigma(3)}, v_{\sigma(4)})$$
	As $\binom{4}{2} = 6$ we have the following big sum:
	\begin{align*}
		(f \wedge g) (v_1,v_2,v_3,v_4) & = f(v_{1}, v_{2})g(v_{3}, v_{4}) + (-1)f(v_{1}, v_{3})g(v_{2}, v_{4}) \\
		                               & + f(v_1,v_4)g(v_2,v_3) + f(v_2,v_3)g(v_1,v_4)                         \\
		                               & + (-1)f(v_2, v_4)g(v_1,v_3) + f(v_3,v_4)g(v_1,v_2)
	\end{align*}
\end{proof}


\begin{customprob}{3.22}
\end{customprob}
\begin{proof}
	It suffices to count the number of inversions, but this is simple, each of the first $l$ elements of the permutation
	have $k$ inversions with the last $k$ elements, yielding $lk$ inversions. As such, $\text{sgn}(\tau) = (-1)^{kl}$.
\end{proof}

\subsection{Problems}
\begin{problem}
\end{problem}

\begin{proof}
	We begin by remembering $\alpha_i: v \to v_i$, so we may write the tensor product
	$$\alpha_i \otimes \alpha_j: (v,w) \to \alpha_i(v)\cdot \alpha_j(w)$$
	So the transformation becomes:
	$$f = \sum_{1 \leq i,j \leq n} g_{ij} \cdot \alpha_i \otimes \alpha_j$$
\end{proof}

\begin{problem}
\end{problem}
\begin{enumerate}[label=(\alph*)]
	\item \begin{proof}
		      This is a simple consequence of the kernel image theorem. We know, from that result, the following identity:
		      $$\dim(V) = \dim(\ker(f)) + \dim(f(V))$$
		      As $f$ is a linear, non-zero, and sends on $\R$, we know $f(V) = \R$, and $\dim(f(V)) = 1$. Substituting back,
		      $\dim(\ker(f)) = n - 1$.
	      \end{proof}
	\item \begin{proof}
		      If $V$ has finite dimension, this is a consequence of the previous item. Being the kernel the same of dimension n-1 ,
		      by taking any vector $v$  such that $f(v) \neq 0$, we may chose $c = g(v)/f(v)$. Now notice $\ker(cf - g)$ has dimension
		      at least $n$, and as such is the whole space. Let's generalize using the first isomorphism theorem. Notice that
		      $$\frac{V}{\ker(f)} = \frac{V}{\ker(g)} \cong \R$$
		      This means that $\frac{V}{\ker(f)}$ and $\frac{V}{\ker(g)}$ are one-dimensional, with the same elements. Take
		      $v$ such that $\bar{v} \neq 0$, choose $c = g(v)/f(v)$, and as before notice that $\ker(cf - g) = V$.
	      \end{proof}
\end{enumerate}

\begin{problem}
\end{problem}
\begin{proof}
	First of linear independence, consider $I$ the set of multi-indices and set $e_I$ as before. Suppose
	$$\sum_{(i_1,\dots,i_k) = I} c_I \alpha_{i_1}\otimes\alpha_{i_2} \cdots \otimes\alpha_{i_n} = 0$$
	Now to uncover each $c_I$, apply the tranformation to $e_I$. If $J \neq I$, then there is a first index
	$j_k \neq i_k$. As such, when applying to $e_I$:
	$$\alpha_{j_1}\otimes \cdots \alpha_{j_k} \cdots \otimes\alpha_{j_n} (e_I) = \alpha_{j_1}(e_{i_1})\cdots \alpha_{j_k}(e_{i_k}) \cdots
		\alpha_{j_n}(e_{i_n}) = 0$$
	As  $\alpha_{j_k}(e_{i_k}) = 0$. And we find:
	$$\Bigg[\sum_{(i_1,\dots,i_k) = I} c_I \cdot \alpha_{i_1}\otimes\alpha_{i_2} \cdots \otimes\alpha_{i_n}\Bigg](e_I) = c_I = 0$$

	Now for span. Notice only by linearity that if two k-linear transformation coincide on the indices, then they coincide for every value.
	As such, given $f \in L_k(V)$, we define $g$ by:
	$$g = \sum_I f(e_I)\cdot\alpha_I$$
	where here $\alpha_I = \alpha_{i_1}\otimes\alpha_{i_2} \cdots \otimes\alpha_{i_n}$.
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	We say $f$ is alternating if, for a permutation $\sigma$:
	$$\sigma f = \text{sgn}(\sigma) f$$

	To show equivalence is to show that  a function has the flipping property, iff it satisfies this permutation property.
	Clearly, permutation implies flipping, as doing any single 2-transposition changes the sign of the permutation. Now, for
	the other side, we record that the sign of a permutation is $(-1)^m$, where $m$ is the number of inversions revelead when
	describing the permutation as 2-transpositions. Following this definition, we see that flipping is sufficient.
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	We may notice, using the wedge products as a basis for the co-vectors that if $f$ is alternating, then $f(v_1, \dots, v_k) = 0$ if
	there are $i \neq j$ with $v_i = v_j$ by looking at the decomposition. But that is actually harder.

	Suppose $f$ is n-alternating and let $v_1, \dots v_n$ be $n$ vectors with $v_i = v_j$ for $i < j$. Then
	after a permutation $\sigma$ that makes $i$ next to $j$, we have:
	$$(\sigma f)(v_1,\dots,v_i,\dots,v_j,\dots,v_n) = \text{sgn}(\sigma) f(v_1, \dots, v_i,v_j, \dots, v_n)$$
	Being $v_i = v_j$
	$$\text{sgn}(\sigma) f(v_1, \dots, v_i,v_j, \dots, v_n) = \text{sgn}(\sigma) f(v_1, \dots, v_j,v_i, \dots, v_n)$$
	But, because $f$ is alternating, we also have:
	$$\text{sgn}(\sigma) f(v_1, \dots, v_i,v_j, \dots, v_n) = - \text{sgn}(\sigma) f(v_1, \dots, v_j,v_i, \dots, v_n)$$
	So both are equal to $0$, and as such
	$f(v_1, \dots, v_n) = 0$.

	Now suppose that whenever two vectors are equal, $f = 0$. Then, given any 2 positions $i < j$ we may write (to simplify, we suppose that
	they are the only ones):
	\begin{equation*}
		0 = f(v_i + v_j, v_i + v_j)  = f(v_i, v_i) + f(v_i, v_j) + f(v_j, v_i) + f(v_j, v_j) = f(v_i,v_j) + f(v_j,v_i)
	\end{equation*}
	So flipping a coordinate changes the sign.
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	$$af \wedge bg = \frac{1}{k!l!}A(af \otimes bg) = \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} (\text{sgn}(\sigma))(\sigma (af\otimes bg))$$
	But we can pop out the constants from the tensor product. Yielding:
	$$\frac{ab}{k!l!} \sum_{\sigma \in S_{k+l}} (\text{sgn}(\sigma))(\sigma (f\otimes g)) = ab(f\wedge g)$$
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	I was thinking of using the relation for covectors $\alpha$:
	$$(\alpha^1 \wedge \alpha^2 \cdots \wedge \alpha^k)(v_1, v_2, \dots, v_k) = \det[\alpha^i(v_j)]$$
	But I don't know how to use it here. I guess we can expand stuff:
	$$\beta^1 \wedge \beta^2 \cdots \wedge \beta^k = \bigwedge_{i = 1}^{k} \sum_{j = 1}^{k} a^i_j \gamma^j$$
	Now, by linearity of the wedge product, we may separate each sum and write the following:
	$$\bigwedge_{i = 1}^{k} \sum_{j = 1}^{k} a^i_j \gamma^j =
		\sum_{\substack{[i_1, i_2, \dots, i_k]\\ \in [k]^k}} (a^1_{i_1} \gamma_{i_1} \wedge a^2_{i_2}\gamma_{i_2} \cdots \wedge a^k_{i_k}\gamma_{i_k})$$
	Now, we know that, because the wedge product is alternating, we only care about permutations of $[k]$, because if we choose two $\gamma_i$'s it will zero.
	As such, we are left with the following, (where the sign comes from the alternating property):
	$$\sum_{\substack{[i_1, i_2, \dots, i_k]\\ \in [k]^k}} (a^1_{i_1} \gamma_{i_1} \wedge a^2_{i_2}\gamma_{i_2} \cdots \wedge a^k_{i_k}\gamma_{i_k}) =
		\sum_{\sigma \in S_k} \text{sgn}(\sigma) \cdot (a^1_{\sigma(1)} \gamma_{1} \wedge a^2_{\sigma(2)}\gamma_{2} \cdots \wedge a^k_{\sigma(k)}\gamma_{k}) $$
	which, by the previous problem is equal to:
	$$\sum_{\sigma \in S_k} \text{sgn}(\sigma) \cdot a^1_{\sigma(1)}a^2_{\sigma(2)}\cdots a^k_{\sigma(k)} \cdot (\gamma_{1} \wedge \gamma_{2} \cdots \wedge \gamma_{k}) =
		(\det A)\, \gamma_{1} \wedge \gamma_{2} \cdots \wedge \gamma_{k}$$
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	This is a collorary from the fact that given a basis $B$ of $V$ and their $\alpha^i$ duals, $\alpha^1 \wedge \alpha^2 \wedge \cdots \wedge \alpha^n$ is a basis for $A_n$. If $\omega$ is a $n$-covector,
	then it is of the form $c(\alpha^1 \wedge \alpha^2 \wedge \cdots \wedge \alpha^n)$, so if it is zero for $B$, then $c = 0$.

	Another way of seeing this is writing $\omega(v_1, v_2, \dots, v_n) = \sum_{\sigma \in S_k} C_\sigma \omega(e_1, e_2, \dots, e_n) = 0$.
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	This seems to have an easy and a hard direction.
	If the covectors are NOT linearly independent, then we may write, say $\alpha^k = \sum_{i=1}^{k-1} c_i \alpha^i$. As such
	$$\alpha_1 \wedge \alpha_2 \cdots \wedge \alpha_k = \sum_{i = 1}^{k-1} c_i \alpha_1 \wedge \alpha_2 \cdots \wedge \alpha_{k-1} \wedge \alpha_i = 0$$

	Now, if they are linearly independent, let's use induction on the determinant formula.
	$$\alpha^1 \wedge \alpha^2 \cdot \wedge \alpha^k(v_1, v_2 \dots, v_k) = \det[\alpha^i (v_j)]$$
	Base case k = 1: $\alpha^1 \neq 0$
	Now, suppose it's valid for $k-1$ and
	Sps for sake of contradiction that $\alpha^1 \wedge \alpha^2 \cdot \wedge \alpha^k = 0$, we may then consider, for a given fixed choice of $w_2,w_3,\dots,w_k$ the linear transformation on $v$
	$$x(v) = \alpha^1 \wedge \alpha^2 \cdots \wedge \alpha^k(v, w_2 \dots, w_k) = \sum_{\sigma \in S_k} \text{sgn}(\sigma)(\sigma \bigotimes_{i = 1}^{k} \alpha^i)(v,w_2, \dots, w_k) = 0$$
	Writing this sum separetely, depending where $v$ is:
	\begin{align*}
		x(v) & = \sum_{\sigma \in S_{k}, \sigma(1) = 1} \alpha^1(v) \cdot \text{sgn}(\sigma)\cdot \alpha^2(w_{\sigma(2)}) \cdots  \alpha^k(w_{\sigma(k)})       \\
		     & + \sum_{\sigma \in S_{k}, \sigma(2) = 1} \alpha^2(v) \cdot \text{sgn}(\sigma)\cdot \alpha^1(w_{\sigma(1)}) \cdots  \alpha^k(w_{\sigma(k)})       \\
		     & \dots                                                                                                                                            \\
		     & + \sum_{\sigma \in S_{k}, \sigma(k) = 1} \alpha^k(v) \cdot \text{sgn}(\sigma)\cdot \alpha^1(w_{\sigma(1)}) \cdots  \alpha^{k-1}(w_{\sigma(k-1)})
	\end{align*}
	If we show the term following $\alpha_1(v)$ is not $0$, then we will have shown linear dependence. We can use induction for this!
	The terms that follows $\alpha_1(v)$ is actually $(\alpha^2(w_2)\wedge \alpha^3(w_3) \cdots \wedge \alpha^k(w_k))$, by the induction hypothesis,
	there is a choice of $w_2, w_3, \dots, w_n$ with the expression non-zero. And we win!
	Reordering, we get that $\alpha^1$ is a linear combination of $\alpha^j$ for $j > 1$.
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	The converse is obvious. Sps $\alpha \wedge \gamma = 0$. Being $\alpha \neq 0$ and $V$ finite dimensional (let's say $n$), we may use it to complete a basis:
	$(\alpha = \alpha^1), \alpha^2, \dots \alpha^n$. Then, we may, using the usual basis for the k-covectors $A_k(V)$, span $\gamma$ by:
	$$\gamma = \sum \gamma(e_I)\alpha_I$$
	Meaning:
	$$\alpha \wedge \gamma = \sum \gamma(e_I) \alpha^1 \wedge \alpha_I = 0$$
	But the non-zero terms $\alpha^1 \wedge \alpha_I$ are L.I in $A_{k+1}(V)$, so that for each of the $\alpha_I$ that do not contain
	$\alpha^1$, $\gamma(e_I) = 0$. As such $\gamma$ contains only basis vectors that contain $\alpha^1$ on the index. That is:
	$$\gamma = \alpha \wedge \Bigg( \sum_{I} \gamma(e_I) \alpha_{I / \{ 1\}} \Bigg)$$
\end{proof}

\section{Differential forms on Rn}

\subsection{Within Text Exercises}

\begin{customprob}{4.3}
\end{customprob}
\begin{proof}
	As seen before, one basis is: $(dx^2 \wedge dx^3 \wedge dx^4)_p$, $(dx^1 \wedge dx^3 \wedge dx^4)_p$, $(dx^1 \wedge dx^2 \wedge dx^4)_p$ and
	$(dx^1 \wedge dx^2 \wedge dx^3)_p$.
\end{proof}

\begin{customprob}{4.4}
\end{customprob}

\begin{proof}
	As seen before, the wedge product is defined point-wise, so at a point $p$ we get:
	$$(\omega \wedge \tau)_p(X_p,Y_p,Z_p) = (\omega_p \wedge \tau_p)(X_p,Y_p,Z_p)$$
	Being $\omega$ a 2-form, $\omega_p$ is a 2-covector, similarly, $\tau_p$ is a 1-covector. By the covector formula (using shuffles) we get
	$$[\omega_p \wedge \tau_p](v_1, v_2, v_3) = \bigg(\omega_p(v_1, v_2)\tau_p(v_3) - \omega_p(v_1, v_3)\tau_p(v_2) + \omega_p(v_2, v_3)\tau_p(v_1) \bigg)$$
	So, as this is valid for all $p$, we may write it as
	$$(\omega \wedge \tau)(X,Y,Z) = \bigg(\omega(X, Y)\tau(Z) - \omega(X, Z)\tau(Y) + \omega(Y, Z)\tau(X) \bigg)$$
\end{proof}

\begin{customprob}{4.9}
\end{customprob}

\begin{proof}
	We need to show that $d\omega = 0$. As $\deg(1/(x^2 + y^2)) = 0$, we have, by the anti-derivation rule:
	$$d\omega = d\bigg(\frac{1}{x^2  +y^2} \bigg) \wedge (-ydx + xdy) + \bigg(\frac{1}{x^2  +y^2} \bigg) d(-y dx + xdy)$$
	Developing the first half:
	$$d\bigg(\frac{1}{x^2  +y^2} \bigg) = - \frac{2xdx + 2ydy}{(x^2 + y^2)^2}$$
	So:
	\begin{align*}
		\bigg(-\frac{2xdx + 2ydy}{(x^2 + y^2)^2}\bigg) \wedge (-ydx + xdy)  =
		\frac{-1}{(x^2 + y^2)^2} & \Bigg( -2xy dx\wedge dx + 2x^2dx \wedge dy     \\
		                         & + -2y^2 dy \wedge dx + 2yx dy \wedge dx \Bigg) \\
	\end{align*}
	Yielding
	$$\frac{-1}{(x^2 + y^2)^2}(2x^2 + 2y^2) dx\wedge dy = \frac{-2}{x^2 + y^2} dx \wedge dy$$
	Now we have to develop the other, easier part. Notice:
	$$d(-y dx + xdy) = - (dy \wedge dx) + dx\wedge dy = 2(dx \wedge dy)$$
	So we get (joining both):
	$$ \frac{-2}{x^2 + y^2} dx \wedge dy +  \frac{2}{x^2 + y^2} dx \wedge dy = 0$$
\end{proof}

\subsection{Problems}

\begin{problem}
\end{problem}

\begin{proof}
	$$\omega(X) = [zdx - dz](y\frac{\partial}{\partial x} + x\frac{\partial}{\partial y}) =
		zy \bigg(dx \frac{\partial}{\partial x}\bigg) + zx \bigg(dx \frac{\partial}{\partial y}\bigg)
		- y \bigg(dz \frac{\partial}{\partial x} \bigg) - x \bigg(dz \frac{\partial}{\partial y} \bigg)$$
	Applying the covectors and cancelling out the zero terms we get:
	$$\omega(X)_{(x,y,z)} = zy$$
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	For this problem it is helpful to remember Prop 3.27. Which states: if $\alpha^1, \alpha^2, \dots \alpha^k$ are covectors, then
	$$[\alpha^1 \wedge \alpha^2 \wedge \cdots \wedge \alpha^k](v_1, v_2, \dots, v_k) = \det [\alpha^i(v_j)]$$
	$\omega$ then becomes:
	$$\omega_p = p^3(dx^1 \wedge dx^2)$$
	To check this is true, we can write out:
	$$[dx^1 \wedge dx^2](a, b) = dx^1(a)dx^2(b) - dx^1(b)dx^2(a) = a^1b^2 - a^2b^1 $$
	As we wanted to show.
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
	We consider $x(r,\theta), y(r, \theta) \in \Omega_0(\R^2)$ as 0-degree covectors. We may then apply $d$ to them
	yielding:
	$$dx_p = d(r\cos(\theta)) = \bigg(\frac{\partial}{\partial r}x dr\bigg) + \bigg(\frac{\partial}{\partial \theta}x d\theta\bigg)
		= \cos(\theta)dr - r\sin(\theta)d\theta$$
	similarly we find:
	$$dy_p =  \sin(\theta)dr + r\cos(\theta)d\theta$$
	We may then calculate $(dx \wedge dy)_p$ as (ignoring the subscript)
	$$dx \wedge dy = \cos(\theta) \sin(\theta) dr \wedge dr + r(\cos(\theta))^2 dr \wedge d\theta
		- r(\sin(\theta))^2 d\theta \wedge dr - r^2\sin(\theta)\cos(\theta)d\theta \wedge d\theta$$
	Cancelling what we can and flipping the wedge product we find:
	$$dx \wedge dy = r dr \wedge d\theta$$
\end{proof}

\begin{problem}
\end{problem}
\begin{proof}
Doing exactly the same thing as the previous problem, we consider $x,y,z$ as 0-covectors and apply the differential.
\begin{align*}
	dx & = \sin(\phi)\cos(\theta)d\rho + \rho\cos(\phi)\cos(\theta)d\phi - \rho\sin(\phi)\sin(\theta) d\theta \\
	dy & = \sin(\phi)\sin(\theta)d\rho + \rho\cos(\phi)\sin(\theta)d\phi + \rho\sin(\phi)\cos(\theta) d\theta \\
	dz & = \cos(\theta)d\rho - \rho\sin(\theta)d\theta                                                        \\
\end{align*}
Now $dx \wedge dy \wedge dz$ is considerably more difficult to calculate, expanding each term linearly and taking only
the non repeating values we may reduce it to:
\begin{align*}
	dx \wedge dy \wedge dz & = \sin(\phi)\cos(\theta)d\rho \wedge \rho\cos(\phi)\sin(\theta)d\phi \wedge (- \rho\sin(\theta)d\theta)   \\
	                       & + \rho\cos(\phi)\cos(\theta)d\phi \wedge \sin(\phi)\sin(\theta)d\rho \wedge (- \rho\sin(\theta)d\theta)   \\
	                       & + \rho\cos(\phi)\cos(\theta)d\phi \wedge \rho\sin(\phi)\cos(\theta) d\theta \wedge \cos(\theta)d\rho      \\
	                       & + (- \rho\sin(\phi)\sin(\theta) d\theta) \wedge \rho\cos(\phi)\sin(\theta)d\phi \wedge  \cos(\theta)d\rho \\
\end{align*}
Now, we may rearrange all products, and pull the functions on front yielding:
\begin{align*}
	dx \wedge dy \wedge dz & = -\rho^2\cos(\phi)\sin(\phi)\cos(\theta)(\sin(\theta))^2  (d\rho \wedge d\phi \wedge d\theta) \\
	                       & + \rho^2\cos(\phi)\sin(\phi)\cos(\theta)(\sin(\theta))^2  (d\rho \wedge d\phi \wedge d\theta)  \\
	                       & + \rho^2\cos(\phi)\sin(\phi)(\cos(\theta))^3 (d\rho \wedge d\phi \wedge d\theta)               \\
	                       & +\rho^2\cos(\phi)\sin(\phi)\cos(\theta)(\sin(\theta))^2 (d\rho \wedge d\phi \wedge d\theta)    \\
\end{align*}
Canceling the first two and using $\cos^3 + \cos \sin^2 = \cos$ on the latter, we find:
$$ dx \wedge dy \wedge dz = \rho^2\cos(\phi)\sin(\phi)\cos(\theta) (d\rho \wedge d\phi \wedge d\theta)$$
\end
{proof}

\begin{problem}
\end{problem}

\begin{proof}
	We ignore products with repeating co-vectors and alternate positions changing signs.
	\begin{align*}
		\alpha \wedge \beta = a_1 b_1 + a_2 b_2 + a_3 b_3 (dx^1 \wedge dx^2 \wedge dx^3 )
	\end{align*}
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	Let's uncover $\alpha \wedge \beta$ and then transform it to a vector following the correspondence.
	\begin{align*}
		\alpha \wedge \beta & = a_1b_2 \alpha^1 \wedge \alpha^2 + a_1b_3 \alpha^1 \wedge \alpha^3                                                                    \\
		                    & + a_2b_1 \alpha^2 \wedge \alpha^1 + a_2b_3 \alpha^2 \wedge \alpha^3                                                                    \\
		                    & + a_3b_1 \alpha^3 \wedge \alpha^1 + a_3b_2 \alpha^3 \wedge \alpha^2                                                                    \\
		                    & = (a_2b_3 - a_3b_2) \alpha^2 \wedge \alpha^3  + (a_1b_3 - a_3b_1) \alpha^1 \wedge \alpha^3 + (a_1b_2 - a_2b_1)\alpha^1 \wedge \alpha^2 \\
		                    & = (a_2b_3 - a_3b_2) \alpha^2 \wedge \alpha^3  - (a_1b_3 - a_3b_1) \alpha^3 \wedge \alpha^1 + (a_1b_2 - a_2b_1)\alpha^1 \wedge \alpha^2 \\
	\end{align*}
	So that:
	$$v_{\alpha \wedge \beta} = \langle (a_2b_3 - a_3b_2), - (a_1b_3 - a_3b_1), (a_1b_2 - a_2b_1)\rangle = v_\alpha \times v_\beta$$
\end{proof}

\begin{problem}
\end{problem}

\begin{proof}
	Notice, firstly, as $D_1$ and $D_2$ are super derivations of degree $m_1$ and $m_2$, then, for any $k$, given $A_k(V)$,
	$D_1(A_k(V)) \subseteq A_{k + m_1}(V)$ and, as such, $D_2(D_1(A_k(V))) \subseteq A_{k + m_1 + m_2}(V)$. Similarly,
	$D_1(D_2(A_k(V))) \subseteq A_{k + m_1 + m_2}(V)$. We need to check the antiderivation property. Calculating $[D_1,D_2]$ on $a \in A_k(V)$ and
	$b \in A_l(V)$:
	Let's expand it by parts, first internaly (using the property inside the arguments):
	$$D_1((D_2a)b +  (-1)^{km_2} a(D_2b)) - (-1)^{m_1m_2} D_2((D_1a)b +  (-1)^{km_1} a(D_1b))$$
	Then on the outer layer:
	\begin{align*}
		[D_1,D_2](a,b) & = [D_1\circ D_2](a) b + (-1)^{(k + m_2)m_1}(D_2a)(D_1b)                       \\
		               & + (-1)^{km_2}((D_1a)(D_2b) + (-1)^{km_1}a[D_1 \circ D_2](b))                  \\
		               & - (-1)^{m_1m_2}([D_2\circ D_1](a) b + (-1)^{(k + m_1)m_2}(D_1a)(D_2b))        \\
		               & - (-1)^{m_1m_2}( (-1)^{km_1} ((D_2a)(D_1b) + (-1)^{km_2}a[D_2 \circ D_1](b)))
	\end{align*}
	Which is absolutely terrible to read, but we may organize it a bit better, cancelling $(-1)^{(k + m_2)m_1}$ with
	$- (-1)^{m_1m_2}( (-1)^{km_1}$; And $(-1)^{km_2}$ with $- (-1)^{m_1m_2}(-1)^{(k + m_1)m_2}$. Yielding
	$$[D_1\circ D_2](a) b + (-1)^{k(m_1 + m_2)}a[D_1 \circ D_2](b) - (-1)^{m_1m_2}\bigg([D_2\circ D_1](a)b + (-1)^{k(m_1 +m_2)}a[D_2 \circ D_1](b) \bigg)$$
	Which is finally equal to:
	$$[D_1, D_2](a) b - (-1)^{k(m_1 + m_2)}a[D_1 \circ D_2](b)$$
\end{proof}
